# ðŸš€ SOTA OVOS Implementation Plan Aâ†’Z
## Building State-of-the-Art Voice Assistant for WASABI EU ENMS

**Project**: Open Voice OS for Energy Management System  
**Goal**: TRUE SOTA - Hybrid Adapt + Qwen3-1.7B with hallucination prevention  
**Timeline**: 6 weeks to production-ready system  
**Date**: November 13, 2025

---

## ðŸŽ¯ Project Vision

Build a **state-of-the-art voice assistant** that:
- âœ… Understands 95%+ of energy management queries
- âœ… Responds in <1s for 80% of queries
- âœ… 99.5%+ accuracy (zero hallucination tolerance)
- âœ… Works 100% offline (no cloud, no GPU required)
- âœ… Handles voice AND text input seamlessly
- âœ… Natural, conversational interaction
- âœ… Industrial-grade reliability

**Why SOTA (State-of-the-Art)?**
- First voice assistant with hybrid intent parsing (Adapt + LLM)
- Hallucination-proof architecture for industrial systems
- Real-time energy monitoring with voice commands
- ISO 50001 compliance through voice
- Multi-language support (English + future expansion)

---

## ðŸ“Š System Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    OVOS CORE PLATFORM                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚   STT       â”‚  â”‚  Wake Word   â”‚  â”‚     TTS      â”‚       â”‚
â”‚  â”‚ (Whisper)   â”‚  â”‚  (Hey OVOS)  â”‚  â”‚  (Piper)     â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ENMS SKILL (Our Implementation)                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  TIER 1: Adapt Parser (Fast Rules)                   â”‚   â”‚
â”‚  â”‚  - 70-80% coverage, <10ms response                   â”‚   â”‚
â”‚  â”‚  - Keyword matching for common queries               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                           â”‚                                  â”‚
â”‚                      [Not matched]                           â”‚
â”‚                           â”‚                                  â”‚
â”‚                           â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  TIER 2: Qwen3-1.7B (Complex Queries)                â”‚   â”‚
â”‚  â”‚  - 15-20% coverage, ~500ms response                  â”‚   â”‚
â”‚  â”‚  - Natural language understanding                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                           â”‚                                  â”‚
â”‚                           â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  VALIDATION LAYER (Hallucination Blocker)            â”‚   â”‚
â”‚  â”‚  - Entity whitelisting                                â”‚   â”‚
â”‚  â”‚  - Confidence scoring                                 â”‚   â”‚
â”‚  â”‚  - API mapping verification                           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                           â”‚                                  â”‚
â”‚                           â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  API EXECUTOR                                         â”‚   â”‚
â”‚  â”‚  - Calls ENMS REST API                               â”‚   â”‚
â”‚  â”‚  - Handles responses                                  â”‚   â”‚
â”‚  â”‚  - Error recovery                                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                           â”‚                                  â”‚
â”‚                           â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  RESPONSE FORMATTER                                   â”‚   â”‚
â”‚  â”‚  - Template-based (no LLM generation)                â”‚   â”‚
â”‚  â”‚  - Voice-optimized output                            â”‚   â”‚
â”‚  â”‚  - Rich responses with context                       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      ENMS API                                â”‚
â”‚  - 90+ endpoints for energy monitoring                       â”‚
â”‚  - Real-time data, anomalies, forecasting                    â”‚
â”‚  - ISO 50001 compliance                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ“… 6-Week Implementation Timeline

### **Phase 0: Setup & Foundation (Week 1)**
**Goal**: Development environment ready, OVOS running, basic skill created

#### Day 1-2: Environment Setup
- [ ] Install OVOS core on development machine
- [ ] Configure STT (Whisper or Vosk)
- [ ] Configure TTS (Piper or Mimic3)
- [ ] Test wake word detection
- [ ] Verify ENMS API access

#### Day 3-4: Skill Scaffold
- [ ] Create ENMS skill structure
- [ ] Setup skill configuration
- [ ] Implement basic intent registration
- [ ] Test "Hello OVOS" skill response
- [ ] Setup logging and debugging

#### Day 5-7: API Integration Layer
- [ ] Create ENMS API client wrapper
- [ ] Implement authentication (if needed)
- [ ] Test all critical endpoints
- [ ] Error handling and retries
- [ ] Connection pooling

**Deliverable**: OVOS running with basic ENMS skill responding to test queries

---

### **Phase 1: Adapt Intent Parser (Week 2)**
**Goal**: 70% query coverage with fast Adapt rules

#### Day 8-9: Core Intents Definition
- [ ] Define 8 core intents:
  - `machine_status` - "Is Compressor-1 running?"
  - `energy_query` - "Show me energy consumption"
  - `power_query` - "What's the current power?"
  - `cost_query` - "How much did we spend?"
  - `comparison` - "Compare X and Y"
  - `ranking` - "Top 3 consumers"
  - `anomaly` - "Any alerts?"
  - `factory_overview` - "System status"

#### Day 10-11: Entity Extraction
- [ ] Machine names (8 machines)
- [ ] Metrics (energy, power, cost, status)
- [ ] Time ranges (today, yesterday, last week, etc.)
- [ ] Numbers (1, 2, 3 for rankings)
- [ ] Test entity extraction accuracy

#### Day 12-14: Intent Handlers
- [ ] Map each intent to API endpoints
- [ ] Implement response formatting
- [ ] Test with 50 sample queries
- [ ] Measure accuracy and speed
- [ ] Refine keyword patterns

**Deliverable**: Working Adapt parser with 70% coverage

---

### **Phase 2: Qwen3-1.7B Integration (Week 3)**
**Goal**: LLM fallback for complex queries

#### Day 15-16: Model Setup
- [ ] Download Qwen3-1.7B-Instruct (GGUF 4-bit)
- [ ] Setup llama.cpp for CPU inference
- [ ] Benchmark inference speed on target hardware
- [ ] Optimize model loading (lazy/eager)
- [ ] Memory management

#### Day 17-18: Prompt Engineering
- [ ] Design system prompt for intent extraction
- [ ] Create few-shot examples
- [ ] Implement JSON schema validation
- [ ] Test with complex queries
- [ ] Optimize for speed (token reduction)

#### Day 19-21: LLM Integration
- [ ] Integrate LLM as Tier 2 fallback
- [ ] Implement timeout handling
- [ ] Add confidence scoring
- [ ] Test hallucination scenarios
- [ ] Performance tuning

**Deliverable**: Hybrid system with LLM fallback working

---

### **Phase 3: Validation Layer (Week 4)**
**Goal**: Hallucination prevention at 99.5%+

#### Day 22-23: Entity Validation
- [ ] Implement machine name validator
- [ ] Fuzzy matching for typos
- [ ] Time range parser and validator
- [ ] Metric validation
- [ ] Intent validation

#### Day 24-25: Whitelist Enforcement
- [ ] Build entity whitelists from ENMS API
- [ ] Auto-refresh from API (daily)
- [ ] Suggestion engine (did you mean?)
- [ ] Test all hallucination scenarios
- [ ] Edge case handling

#### Day 26-28: Integration Testing
- [ ] Test all 118 queries from test-questions.md
- [ ] Measure hallucination prevention rate
- [ ] Test validation rejection flow
- [ ] Clarification dialog testing
- [ ] Performance profiling

**Deliverable**: Validated system with 99.5%+ accuracy

---

### **Phase 4: Response Generation & UX (Week 5)**
**Goal**: Natural, voice-optimized responses

#### Day 29-30: Template System
- [ ] Design response templates for all intents
- [ ] Voice-optimized formatting
- [ ] Number pronunciation (kWh, kW)
- [ ] Time formatting (friendly dates)
- [ ] Rich responses with context

#### Day 31-32: Conversation Context
- [ ] Session state management
- [ ] Follow-up question handling
- [ ] Context carryover ("What about Boiler-1?")
- [ ] Clarification dialogs
- [ ] Error recovery

#### Day 33-35: UI/UX Polish
- [ ] Voice feedback ("Let me check...")
- [ ] Progress indicators for slow queries
- [ ] Error messages (friendly, helpful)
- [ ] Confirmation flows
- [ ] Help system ("What can you do?")

**Deliverable**: Polished voice UX with natural responses

---

### **Phase 5: Testing & Optimization (Week 6)**
**Goal**: Production-ready with comprehensive testing

#### Day 36-37: Unit Testing
- [ ] Test all intent parsers
- [ ] Test validation layer
- [ ] Test API client
- [ ] Test response formatter
- [ ] 90%+ code coverage

#### Day 38-39: Integration Testing
- [ ] End-to-end query testing
- [ ] Multi-turn conversation testing
- [ ] Error scenario testing
- [ ] Performance benchmarking
- [ ] Load testing

#### Day 40-41: User Acceptance Testing
- [ ] 10 real users Ã— 20 queries each
- [ ] Collect feedback
- [ ] Measure accuracy rate
- [ ] Measure response time
- [ ] Usability scoring

#### Day 42: Production Deployment
- [ ] Deploy to production hardware
- [ ] Configure monitoring
- [ ] Setup logging
- [ ] Documentation
- [ ] Training materials

**Deliverable**: Production-ready SOTA OVOS system

---

## ðŸ› ï¸ Technical Stack

### OVOS Components
```yaml
ovos-core: Latest stable (2024.11+)
STT: ovos-stt-plugin-whisper (or Vosk for faster)
TTS: ovos-tts-plugin-piper (or Mimic3)
Wake Word: ovos-ww-plugin-precise (or Porcupine)
```

### LLM Components
```yaml
Model: Qwen/Qwen3-1.7B-Instruct-GGUF (Q4_K_M)
Inference: llama-cpp-python
Size: ~1.2GB on disk, 2-3GB RAM
Speed: 400-600ms on 4-core CPU
```

### Python Stack
```yaml
Python: 3.10+
OVOS Framework: ovos-workshop
HTTP Client: httpx (async support)
Validation: pydantic (data validation)
Testing: pytest + pytest-asyncio
```

### Development Tools
```yaml
IDE: VSCode with OVOS extension
Version Control: Git
CI/CD: GitHub Actions
Monitoring: Prometheus + Grafana (optional)
```

---

## ðŸ“‚ Project Structure

```
enms-ovos-skill/
â”œâ”€â”€ __init__.py                    # Skill entry point
â”œâ”€â”€ skill.json                     # Skill metadata
â”œâ”€â”€ requirements.txt               # Dependencies
â”œâ”€â”€ README.md                      # Documentation
â”‚
â”œâ”€â”€ intent/                        # Intent definitions
â”‚   â”œâ”€â”€ machine_status.intent      # Adapt patterns
â”‚   â”œâ”€â”€ energy_query.intent
â”‚   â”œâ”€â”€ cost_query.intent
â”‚   â”œâ”€â”€ comparison.intent
â”‚   â”œâ”€â”€ ranking.intent
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ entities/                      # Entity definitions
â”‚   â”œâ”€â”€ machine.entity             # Machine names
â”‚   â”œâ”€â”€ metric.entity              # Metrics
â”‚   â”œâ”€â”€ time_range.entity          # Time expressions
â”‚   â””â”€â”€ number.entity              # Numbers
â”‚
â”œâ”€â”€ locale/                        # Localization
â”‚   â””â”€â”€ en-us/                     # English (US)
â”‚       â”œâ”€â”€ dialog/                # Response templates
â”‚       â”‚   â”œâ”€â”€ machine_status.dialog
â”‚       â”‚   â”œâ”€â”€ energy_query.dialog
â”‚       â”‚   â””â”€â”€ ...
â”‚       â””â”€â”€ vocab/                 # Vocabulary
â”‚           â”œâ”€â”€ keyword.voc        # Keywords
â”‚           â””â”€â”€ ...
â”‚
â”œâ”€â”€ lib/                           # Core libraries
â”‚   â”œâ”€â”€ intent_parser.py           # Hybrid parser
â”‚   â”‚   â”œâ”€â”€ AdaptParser
â”‚   â”‚   â”œâ”€â”€ Qwen3Parser
â”‚   â”‚   â””â”€â”€ HybridParser
â”‚   â”‚
â”‚   â”œâ”€â”€ validator.py               # Validation layer
â”‚   â”‚   â”œâ”€â”€ ENMSValidator
â”‚   â”‚   â”œâ”€â”€ EntityValidator
â”‚   â”‚   â””â”€â”€ ConfidenceScorer
â”‚   â”‚
â”‚   â”œâ”€â”€ api_client.py              # ENMS API client
â”‚   â”‚   â”œâ”€â”€ ENMSClient
â”‚   â”‚   â”œâ”€â”€ EndpointMapper
â”‚   â”‚   â””â”€â”€ ErrorHandler
â”‚   â”‚
â”‚   â”œâ”€â”€ response_formatter.py     # Response generation
â”‚   â”‚   â”œâ”€â”€ TemplateEngine
â”‚   â”‚   â”œâ”€â”€ VoiceOptimizer
â”‚   â”‚   â””â”€â”€ ContextManager
â”‚   â”‚
â”‚   â””â”€â”€ utils.py                   # Utilities
â”‚       â”œâ”€â”€ time_parser.py
â”‚       â”œâ”€â”€ number_formatter.py
â”‚       â””â”€â”€ logger.py
â”‚
â”œâ”€â”€ models/                        # LLM models
â”‚   â””â”€â”€ qwen3-1.7b-instruct-q4.gguf
â”‚
â”œâ”€â”€ config/                        # Configuration
â”‚   â”œâ”€â”€ enms_api.yaml              # API settings
â”‚   â”œâ”€â”€ llm_config.yaml            # LLM settings
â”‚   â””â”€â”€ validation.yaml            # Validation rules
â”‚
â”œâ”€â”€ tests/                         # Test suite
â”‚   â”œâ”€â”€ test_adapt_parser.py
â”‚   â”œâ”€â”€ test_llm_parser.py
â”‚   â”œâ”€â”€ test_validator.py
â”‚   â”œâ”€â”€ test_api_client.py
â”‚   â”œâ”€â”€ test_integration.py
â”‚   â””â”€â”€ test_queries.yaml          # 118 test queries
â”‚
â””â”€â”€ docs/                          # Documentation
    â”œâ”€â”€ API_REFERENCE.md
    â”œâ”€â”€ USER_GUIDE.md
    â”œâ”€â”€ DEVELOPMENT.md
    â””â”€â”€ ARCHITECTURE.md
```

---

## ðŸ”§ Detailed Implementation Guides

### 1. OVOS Skill Entry Point

```python
# __init__.py
from ovos_workshop.skills import OVOSSkill
from ovos_workshop.decorators import intent_handler
from .lib.intent_parser import HybridParser
from .lib.validator import ENMSValidator
from .lib.api_client import ENMSClient
from .lib.response_formatter import ResponseFormatter

class ENMSSkill(OVOSSkill):
    """
    SOTA Energy Management Voice Assistant
    Hybrid Adapt + Qwen3-1.7B with hallucination prevention
    """
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.parser = None
        self.validator = None
        self.api_client = None
        self.formatter = None
    
    def initialize(self):
        """Initialize skill components"""
        self.log.info("ðŸš€ Initializing ENMS SOTA Skill...")
        
        # Load configuration
        api_config = self.settings.get('enms_api', {})
        llm_config = self.settings.get('llm', {})
        
        # Initialize components
        self.api_client = ENMSClient(
            base_url=api_config.get('base_url', 'http://localhost:8001'),
            timeout=api_config.get('timeout', 30)
        )
        
        self.validator = ENMSValidator(
            api_client=self.api_client
        )
        
        self.parser = HybridParser(
            llm_model_path=llm_config.get('model_path'),
            llm_enabled=llm_config.get('enabled', True)
        )
        
        self.formatter = ResponseFormatter()
        
        self.log.info("âœ… ENMS Skill initialized successfully")
    
    @intent_handler('machine_status.intent')
    def handle_machine_status(self, message):
        """Handle: Is Compressor-1 running?"""
        machine = message.data.get('machine')
        self._process_query(message.utterances[0], intent='machine_status', machine=machine)
    
    @intent_handler('energy_query.intent')
    def handle_energy_query(self, message):
        """Handle: Show me energy consumption"""
        machine = message.data.get('machine')
        time_range = message.data.get('time_range', 'today')
        self._process_query(message.utterances[0], intent='energy_query', machine=machine, time=time_range)
    
    # ... more intent handlers
    
    def _process_query(self, utterance, **context):
        """
        Main query processing pipeline
        1. Parse with hybrid system
        2. Validate output
        3. Call API
        4. Format response
        5. Speak
        """
        try:
            # Show thinking indicator for slow queries
            self.speak_dialog("checking", wait=False)
            
            # Parse intent
            parsed = self.parser.parse(utterance, context=context)
            
            if not parsed['safe']:
                # Validation failed - ask clarification
                self.speak(parsed['clarification'])
                return
            
            # Call ENMS API
            api_result = self.api_client.execute(parsed['result'])
            
            # Format response
            response = self.formatter.format(
                intent=parsed['result']['intent'],
                data=api_result,
                voice_optimized=True
            )
            
            # Speak result
            self.speak(response)
            
        except Exception as e:
            self.log.error(f"Error processing query: {e}")
            self.speak_dialog("error.general")
    
    def converse(self, message):
        """
        Handle follow-up questions and multi-turn conversation
        """
        # Check if this is a follow-up to previous query
        if self._is_followup(message):
            return self._handle_followup(message)
        
        return False  # Let other skills handle it
    
    def stop(self):
        """Clean shutdown"""
        if self.parser:
            self.parser.cleanup()
        return True
```

### 2. Hybrid Intent Parser

```python
# lib/intent_parser.py
from typing import Dict, Optional
import time
from .adapt_parser import AdaptParser
from .qwen3_parser import Qwen3Parser
from .validator import ENMSValidator

class HybridParser:
    """
    SOTA Hybrid Intent Parser
    Tier 1: Adapt (fast, 70-80% coverage)
    Tier 2: Qwen3-1.7B (complex, 15-20% coverage)
    Tier 3: Validation (99.5% hallucination prevention)
    """
    
    def __init__(self, llm_model_path: str, llm_enabled: bool = True):
        self.adapt = AdaptParser()
        self.llm = Qwen3Parser(model_path=llm_model_path) if llm_enabled else None
        self.validator = ENMSValidator()
        
        # Metrics
        self.stats = {
            'adapt_hits': 0,
            'llm_hits': 0,
            'validation_rejections': 0,
            'total_queries': 0
        }
    
    def parse(self, utterance: str, context: Dict = None) -> Dict:
        """
        Parse utterance with hybrid approach
        
        Returns:
            {
                'safe': bool,
                'result': Dict or None,
                'source': 'adapt' | 'llm',
                'latency_ms': float,
                'clarification': str (if unsafe)
            }
        """
        start_time = time.time()
        self.stats['total_queries'] += 1
        
        # Tier 1: Try Adapt first
        adapt_result = self.adapt.parse(utterance, context)
        
        if adapt_result['matched']:
            self.stats['adapt_hits'] += 1
            latency = (time.time() - start_time) * 1000
            
            return {
                'safe': True,
                'result': adapt_result['intent_data'],
                'source': 'adapt',
                'latency_ms': latency,
                'confidence': 1.0  # Adapt is deterministic
            }
        
        # Tier 2: Fallback to LLM
        if self.llm is None:
            return {
                'safe': False,
                'result': None,
                'source': 'none',
                'clarification': "I didn't understand that. Could you rephrase?"
            }
        
        llm_result = self.llm.parse(utterance, context)
        self.stats['llm_hits'] += 1
        
        # Tier 3: Validate LLM output
        validation = self.validator.validate_llm_output(llm_result)
        
        if not validation['valid']:
            self.stats['validation_rejections'] += 1
            latency = (time.time() - start_time) * 1000
            
            return {
                'safe': False,
                'result': None,
                'source': 'llm',
                'latency_ms': latency,
                'clarification': validation['clarification_needed'],
                'errors': validation['errors']
            }
        
        # Success
        latency = (time.time() - start_time) * 1000
        
        return {
            'safe': True,
            'result': validation['validated_output'],
            'source': 'llm',
            'latency_ms': latency,
            'confidence': llm_result.get('confidence', 0.0)
        }
    
    def get_stats(self) -> Dict:
        """Get performance statistics"""
        total = self.stats['total_queries']
        if total == 0:
            return self.stats
        
        return {
            **self.stats,
            'adapt_percentage': (self.stats['adapt_hits'] / total) * 100,
            'llm_percentage': (self.stats['llm_hits'] / total) * 100,
            'rejection_rate': (self.stats['validation_rejections'] / total) * 100
        }
    
    def cleanup(self):
        """Clean shutdown"""
        if self.llm:
            self.llm.unload_model()
```

### 3. Qwen3 Parser Implementation

```python
# lib/qwen3_parser.py
from llama_cpp import Llama
import json
from typing import Dict

class Qwen3Parser:
    """
    Qwen3-1.7B LLM intent parser
    Outputs structured JSON for validation
    """
    
    def __init__(self, model_path: str):
        self.model = None
        self.model_path = model_path
        self._load_model()
    
    def _load_model(self):
        """Load Qwen3 model with optimizations"""
        self.model = Llama(
            model_path=self.model_path,
            n_ctx=2048,           # Context window
            n_threads=4,          # CPU threads
            n_gpu_layers=0,       # CPU only
            use_mmap=True,        # Memory-mapped file
            use_mlock=False,      # Don't lock in RAM
            verbose=False
        )
    
    def parse(self, utterance: str, context: Dict = None) -> Dict:
        """
        Parse utterance to structured intent
        
        Returns:
            {
                'intent': str,
                'entities': Dict,
                'confidence': float
            }
        """
        # Build prompt
        system_prompt = self._build_system_prompt()
        user_message = f"User query: {utterance}"
        
        if context:
            user_message += f"\nContext: {json.dumps(context)}"
        
        prompt = f"""<|im_start|>system
{system_prompt}<|im_end|>
<|im_start|>user
{user_message}<|im_end|>
<|im_start|>assistant
"""
        
        # Generate response
        response = self.model(
            prompt,
            max_tokens=200,
            temperature=0.1,      # Low temp for consistency
            top_p=0.9,
            stop=["<|im_end|>"],
            echo=False
        )
        
        # Parse JSON output
        try:
            output_text = response['choices'][0]['text'].strip()
            parsed = json.loads(output_text)
            
            # Add confidence score if not present
            if 'confidence' not in parsed:
                parsed['confidence'] = self._estimate_confidence(parsed)
            
            return parsed
            
        except json.JSONDecodeError:
            # LLM failed to output valid JSON
            return {
                'intent': 'unknown',
                'entities': {},
                'confidence': 0.0,
                'error': 'Invalid JSON output'
            }
    
    def _build_system_prompt(self) -> str:
        """System prompt for intent extraction"""
        return """You are an intent parser for an Energy Management System.

Extract the user's intent and entities in JSON format ONLY.

Available intents:
- energy_query: User wants energy consumption data
- machine_status: User wants machine status
- comparison: User wants to compare machines
- ranking: User wants top/bottom machines
- anomaly: User wants anomaly/alert information
- cost_query: User wants cost information
- forecast: User wants energy forecast
- factory_overview: User wants overall system status

Available machines:
Compressor-1, Compressor-EU-1, Boiler-1, HVAC-Main, HVAC-EU-North, Conveyor-A, Injection-Molding-1, Pump-1

Available metrics:
energy_kwh, power_kw, status, cost_usd, anomalies, efficiency, production

Time ranges: today, yesterday, last 24 hours, last week, this month, or specific dates

Output MUST be valid JSON:
{
  "intent": "<intent_name>",
  "entities": {
    "machine": "<machine_name or null>",
    "metric": "<metric or null>",
    "time_range": "<time_expression or null>",
    "limit": <number or null>
  },
  "confidence": <0.0-1.0>
}

Do NOT generate explanations or narratives. ONLY output the JSON object."""
    
    def _estimate_confidence(self, parsed: Dict) -> float:
        """Estimate confidence based on output completeness"""
        score = 0.5  # Base score
        
        if parsed.get('intent'):
            score += 0.2
        
        entities = parsed.get('entities', {})
        if entities.get('machine'):
            score += 0.15
        if entities.get('metric'):
            score += 0.15
        
        return min(score, 1.0)
    
    def unload_model(self):
        """Unload model from memory"""
        if self.model:
            del self.model
            self.model = None
```

---

## ðŸ“Š Success Metrics & KPIs

### Performance Targets

| Metric | Target | How to Measure |
|--------|--------|----------------|
| **Query Coverage** | 95%+ | Test with 118 queries from test-questions.md |
| **Response Time (Fast)** | <100ms | 80% of queries via Adapt |
| **Response Time (Slow)** | <1s | 20% of queries via LLM |
| **Accuracy** | 99.5%+ | Correct intent + entities + API call |
| **Hallucination Rate** | <0.5% | Validation rejection + manual review |
| **User Satisfaction** | >4.5/5 | User testing with 10+ users |
| **Uptime** | 99%+ | 24/7 operation without crashes |

### Testing Methodology

```yaml
Unit Tests:
  - Intent parser: 100 test cases
  - Validator: 50 test cases
  - API client: 30 test cases
  - Coverage target: 90%+

Integration Tests:
  - 118 queries from test-questions.md
  - All must pass with correct responses
  - Performance within targets

User Acceptance Testing:
  - 10 users Ã— 20 queries each = 200 queries
  - Measure: accuracy, speed, satisfaction
  - Collect qualitative feedback

Stress Testing:
  - 100 queries/minute sustained
  - Memory usage <4GB
  - No memory leaks over 24h
```

---

## ðŸš€ Deployment Checklist

### Pre-Deployment
- [ ] All tests passing (unit + integration)
- [ ] Performance benchmarks met
- [ ] User acceptance criteria passed
- [ ] Documentation complete
- [ ] Security review done

### Production Setup
- [ ] Install OVOS on target hardware
- [ ] Configure system services (systemd)
- [ ] Setup monitoring (logs, metrics)
- [ ] Configure backups
- [ ] Test failover scenarios

### Post-Deployment
- [ ] Monitor error rates (first 24h)
- [ ] Collect user feedback
- [ ] Performance tuning
- [ ] Bug fixes as needed
- [ ] Feature requests backlog

---

## ðŸ“š Documentation Requirements

### Technical Documentation
1. **Architecture Guide** - System design and components
2. **API Reference** - All functions and classes
3. **Configuration Guide** - Settings and environment variables
4. **Deployment Guide** - Installation and setup
5. **Troubleshooting Guide** - Common issues and solutions

### User Documentation
1. **Quick Start Guide** - Getting started in 5 minutes
2. **User Manual** - Complete feature reference
3. **Query Examples** - 100+ example queries
4. **FAQ** - Frequently asked questions
5. **Video Tutorials** - Screen recordings

---

## ðŸ”„ Maintenance & Evolution

### Weekly
- Monitor error logs
- Check performance metrics
- User feedback review
- Minor bug fixes

### Monthly
- Update machine whitelist from API
- Performance optimization
- New feature development
- Model fine-tuning (if needed)

### Quarterly
- Major version update
- New feature releases
- User training sessions
- Architecture review

---

## ðŸŽ¯ Success Definition

**The system is SOTA when**:
1. âœ… Users prefer voice over dashboard for 50%+ of queries
2. âœ… Query accuracy is 99%+ in production
3. âœ… Response time feels "instant" (subjective <500ms)
4. âœ… Zero hallucination incidents in production
5. âœ… System runs 24/7 without manual intervention
6. âœ… Users say "it just works" in feedback

---

## ðŸš€ Let's Build This!

**Next Steps**:
1. Review this plan and approve/adjust
2. Setup development environment (Day 1)
3. Start Phase 0 implementation
4. Daily standups to track progress
5. Weekly demos to stakeholders

**Resources Needed**:
- Development machine (8GB+ RAM, 4+ CPU cores)
- Access to ENMS API (localhost or staging)
- Qwen3-1.7B model download (~1.2GB)
- Time: 6 weeks full-time or 12 weeks part-time

**Let's make this the best voice assistant for industrial energy management! ðŸš€**

---

**Questions? Need clarification on any phase? Ready to start coding? Let me know!**
