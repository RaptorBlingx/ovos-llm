# OVOS Intent Parser Analysis: Adapt vs LLM
## For WASABI EU Project Energy Management System

**Author**: AI Technical Analysis  
**Date**: November 13, 2025  
**Context**: ENMS API integration with OVOS voice assistant

---

## ðŸ“‹ Executive Summary

**RECOMMENDATION**: **Hybrid Approach (Adapt + Small LLM Fallback)**

For WASABI's ENMS integration with OVOS, I recommend using **Adapt as the primary intent parser** with a **small LLM (Qwen2.5 1.5B/3B) as fallback** for complex queries. This provides the best balance of speed, accuracy, and resource efficiency without GPU requirements.

**Key Reasoning**:
- 70-80% of queries can be handled by fast Adapt rules (<10ms)
- Complex/ambiguous queries fallback to LLM (~500ms-1s on CPU)
- Maintains accuracy while optimizing for speed
- No GPU required
- Fully open-source and local

---

## ðŸŽ¯ Understanding the Context

### WASABI EU Project
The WASABI project focuses on **energy management and optimization in industrial manufacturing**. From your ENMS API documentation, the system:
- Monitors 8 machines across multiple factories
- Tracks real-time energy consumption (kWh, kW)
- Detects anomalies using ML (Isolation Forest)
- Provides ISO 50001 compliance reporting
- Offers ~90 REST API endpoints for data access

### OVOS (Open Voice OS)
**Open Voice OS** is a fully open-source voice assistant platform:
- **Privacy-first**: Runs 100% locally, no cloud required
- **Open-source**: Apache/GPL licensed, community-driven
- **Modular**: Plugin-based architecture (skills, TTS, STT)
- **Resource-efficient**: Runs on Raspberry Pi 4, x86 systems
- **Flexible**: Supports voice + text input
- **Successor to Mycroft AI**: Active development, growing community

**Architecture**:
```
User Input (Voice/Text)
    â†“
STT (Speech-to-Text) [if voice]
    â†“
Intent Parser â† [YOUR DECISION HERE]
    â†“
Skill Handler (ENMS API calls)
    â†“
TTS (Text-to-Speech)
    â†“
Response to User
```

---

## ðŸ” Your Use Case Analysis

### Query Complexity Breakdown (from test-questions.md)

**Simple Queries (40%)** - Adapt excels:
- "Compressor-1 power" â†’ Single machine, single metric
- "Boiler status" â†’ Single machine, single intent
- "top 3" â†’ Ranking with number

**Medium Queries (35%)** - Adapt can handle with rules:
- "How much energy did Compressor-1 use in the last 24 hours?"
- "Compare Compressor-1 and Boiler-1"
- "Cost analysis for this month"

**Complex Queries (25%)** - LLM shines:
- "Show me the energy consumption of Compressor-1 from October 27, 3 PM to October 28, 10 AM"
- "What's the average energy consumption during peak hours (8 AM to 6 PM) for the last 3 days?"
- "Which machines have had the most anomalies this month?"
- "Compare energy consumption vs baseline with trend analysis"

### Critical Requirements
From your documentation:
1. **Accuracy**: Must map queries to correct API endpoints
2. **Speed**: Voice assistant needs <1s response for good UX
3. **No GPU**: Must run on CPU-only systems
4. **Local/Offline**: Privacy requirement (WASABI EU project)
5. **Multi-entity extraction**: Machine names, time ranges, metrics
6. **Ambiguity handling**: "top 3", "watts" without context

---

## âš–ï¸ Detailed Comparison

### Option 1: Adapt Intent Parser

**What is Adapt?**
- Lightweight keyword-based intent parser from Mycroft
- Rule-based pattern matching with keywords and entities
- Deterministic behavior (same input = same output)

**Architecture Example**:
```python
# Define intent with keywords and entities
class EnergyQueryIntent(IntentBuilder):
    .require("EnergyKeyword")  # "power", "energy", "consumption"
    .require("MachineEntity")   # "Compressor-1", "Boiler-1"
    .optionally("TimeRange")    # "yesterday", "last week"
```

#### âœ… Adapt Pros
1. **Blazing Fast**: 5-10ms response time
2. **Low Memory**: ~50MB RAM usage
3. **CPU-Only**: No GPU required, runs on Pi 4
4. **Predictable**: Deterministic, easy to debug
5. **Mature**: Battle-tested in Mycroft/OVOS
6. **Offline**: 100% local, no external dependencies
7. **Resource Efficient**: Can handle hundreds of requests/sec
8. **Native OVOS**: Built-in, no extra setup

#### âŒ Adapt Cons
1. **Limited NLU**: Poor with complex natural language
2. **Manual Configuration**: Must define every intent pattern
3. **Rigid**: Struggles with query variations
4. **Ambiguity Issues**: "top 3" without context is hard
5. **Maintenance**: Need to update keywords for new patterns
6. **No Context Understanding**: Can't infer intent from conversation
7. **Entity Extraction**: Basic regex/keyword matching only

#### Adapt Suitability for Your Queries
- âœ… **EXCELLENT**: "Compressor-1 power", "Boiler status", "cost today"
- âš ï¸ **MODERATE**: "Compare X and Y", "energy last 24 hours"
- âŒ **POOR**: "Show average during peak hours for 3 days", "trend analysis"

**Estimated Coverage**: 40-50% of your test queries work well out-of-box, 70-80% with extensive configuration

---

### Option 2: Small LLM (Qwen2.5 1.5B-3B)

**What is it?**
- Small language model fine-tuned for instruction following
- Quantized to 4-bit/8-bit for CPU inference
- Understands natural language and extracts structured data

**Architecture Example**:
```python
# System prompt defines task
prompt = """You are an intent parser for an energy management system.
Extract: intent, machine_name, metric, time_range
User: "How much energy did Compressor-1 use yesterday?"
Output: {"intent": "energy_query", "machine": "Compressor-1", "metric": "energy_kwh", "time": "yesterday"}
"""
```

#### âœ… LLM Pros
1. **Better NLU**: Understands complex natural language
2. **Flexible**: Handles query variations automatically
3. **Less Configuration**: Prompt engineering vs manual rules
4. **Context Awareness**: Can maintain conversation context
5. **Multi-Entity**: Extracts multiple entities accurately
6. **Ambiguity Handling**: Uses context to disambiguate
7. **Generalization**: Works with unseen query patterns
8. **Instruction Following**: Can output structured JSON

#### âŒ LLM Cons
1. **Slower**: 500ms-2s on CPU (with quantization)
2. **Higher Memory**: 2-4GB RAM (4-bit quantization)
3. **Unpredictable**: Non-deterministic outputs
4. **Hallucination Risk**: May invent data or APIs
5. **Harder to Debug**: Black box behavior
6. **Prompt Engineering**: Requires careful prompt design
7. **Model Size**: 1.5B params = ~1GB disk space
8. **CPU Intensive**: Sustained load may slow system

#### LLM Performance Benchmarks (CPU)
Based on typical hardware:

| Model | Size | RAM | CPU Inference | Quality |
|-------|------|-----|---------------|---------|
| Qwen2.5-1.5B-Q4 | 1GB | 2GB | 500-800ms | Good |
| Qwen2.5-3B-Q4 | 2GB | 3GB | 1-1.5s | Better |
| Qwen2.5-7B-Q4 | 4GB | 6GB | 3-5s | Best âŒ Too slow |

**Note**: Qwen2.5 1.5B is currently one of the best small models for instruction following

#### LLM Suitability for Your Queries
- âœ… **EXCELLENT**: All complex queries, ambiguous queries
- âœ… **GOOD**: Multi-entity extraction, time parsing
- âš ï¸ **MODERATE**: May be overkill for simple queries
- âŒ **POOR**: Not for latency-critical real-time apps

**Estimated Coverage**: 95%+ of your test queries

---

## ðŸŽ¯ Recommendation: Hybrid Approach

### Architecture: Tiered Intent Parsing

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    User Query (Voice/Text)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1ï¸âƒ£ TIER 1: Fast Adapt Rules       â”‚
â”‚     - Simple queries (1 machine)     â”‚
â”‚     - Status checks                  â”‚
â”‚     - Direct metrics                 â”‚
â”‚     âš¡ <10ms response                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
          [Not matched]
               â”‚
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2ï¸âƒ£ TIER 2: Heuristic Router       â”‚
â”‚     - "top 3" â†’ ranking intent      â”‚
â”‚     - "compare" â†’ comparison         â”‚
â”‚     - time keywords â†’ temporal       â”‚
â”‚     âš¡ <5ms routing                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
          [Ambiguous/Complex]
               â”‚
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3ï¸âƒ£ TIER 3: LLM Fallback (Qwen2.5) â”‚
â”‚     - Complex temporal parsing       â”‚
â”‚     - Multi-entity extraction        â”‚
â”‚     - Ambiguous queries              â”‚
â”‚     ðŸ• 500ms-1s response             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Implementation Strategy

#### Step 1: Adapt for Common Patterns (70-80% coverage)
```python
# Define core intents
INTENTS = {
    "machine_status": ["status", "online", "running"],
    "energy_query": ["energy", "kwh", "consumption", "power", "watts"],
    "cost_query": ["cost", "price", "expense"],
    "comparison": ["compare", "vs", "versus"],
    "ranking": ["top", "highest", "best", "worst"],
    "anomaly": ["anomaly", "alert", "unusual", "problem"],
}

# Entity extraction
ENTITIES = {
    "machine": ["Compressor-1", "Boiler-1", "HVAC-Main", ...],
    "time_range": ["today", "yesterday", "last 24 hours", ...],
    "metric": ["kwh", "kw", "watts", "energy", "power"],
}
```

#### Step 2: Heuristic Router (15% coverage)
```python
def heuristic_router(query):
    """Fast pattern matching before LLM"""
    query_lower = query.lower()
    
    # Top N queries
    if re.match(r'\b(top|highest)\s+\d+', query_lower):
        return Intent("ranking", number=extract_number(query))
    
    # Comparison queries
    if "compare" in query_lower or " vs " in query_lower:
        machines = extract_machines(query)
        return Intent("comparison", machines=machines)
    
    # Factory-wide queries
    if any(w in query_lower for w in ["factory", "total", "all machines"]):
        return Intent("factory_overview")
    
    return None  # Fall through to LLM
```

#### Step 3: LLM Fallback (10-15% coverage)
```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# Load Qwen2.5-1.5B-Instruct (4-bit quantized)
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    load_in_4bit=True,
    device_map="cpu"
)

SYSTEM_PROMPT = """You are an intent parser for ENMS (Energy Management System).
Available intents: energy_query, machine_status, comparison, ranking, anomaly, cost_query, forecast
Machines: Compressor-1, Boiler-1, HVAC-Main, Conveyor-A, Injection-Molding-1, Compressor-EU-1, HVAC-EU-North
Metrics: energy_kwh, power_kw, status, cost_usd, anomalies

Return JSON only:
{
  "intent": "<intent_name>",
  "entities": {
    "machine": "<machine_name or null>",
    "metric": "<metric or null>",
    "time_range": {"start": "ISO8601", "end": "ISO8601"} or "relative_time",
    "limit": <number or null>
  }
}
"""

def llm_parse(user_query):
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": user_query}
    ]
    response = model.generate(messages, max_tokens=200)
    return json.loads(response)
```

### Benefits of Hybrid Approach

1. **Best of Both Worlds**
   - Speed: 80% of queries <10ms (Adapt)
   - Accuracy: 95%+ coverage (Adapt + LLM)
   - Resource efficient: LLM only for complex cases

2. **Graceful Degradation**
   - If LLM fails/slow, fall back to clarification question
   - Adapt handles critical queries even if LLM crashes

3. **Cost-Effective Development**
   - Start with Adapt for 80% coverage (1-2 weeks)
   - Add LLM for remaining 20% (1 week)
   - Iterate based on real usage patterns

4. **Monitoring & Optimization**
   - Track which tier handles each query
   - Promote common LLM patterns to Adapt rules
   - Reduce LLM usage over time

---

## ðŸ“Š Comparison Matrix

| Criteria | Adapt Only | LLM Only | **Hybrid (Recommended)** |
|----------|------------|----------|--------------------------|
| **Speed** | âš¡ <10ms | ðŸŒ 500ms-1s | âš¡ <10ms (80%), 500ms (20%) |
| **Accuracy (Simple)** | âœ… 95% | âœ… 98% | âœ… 95% |
| **Accuracy (Complex)** | âŒ 40% | âœ… 95% | âœ… 95% |
| **Memory** | âœ… 50MB | âš ï¸ 3GB | âš ï¸ 3GB |
| **CPU Usage** | âœ… Low | âŒ High | âš ï¸ Medium |
| **No GPU Required** | âœ… Yes | âœ… Yes (with quantization) | âœ… Yes |
| **Offline/Local** | âœ… Yes | âœ… Yes | âœ… Yes |
| **Development Time** | âš ï¸ 3-4 weeks | âš¡ 1-2 weeks | âš ï¸ 2-3 weeks |
| **Maintenance** | âŒ High | âœ… Low | âš ï¸ Medium |
| **Debuggability** | âœ… Easy | âŒ Hard | âš ï¸ Medium |
| **Open Source** | âœ… Yes | âœ… Yes | âœ… Yes |
| **Overall Score** | 6.5/10 | 7.5/10 | **9/10** â­ |

---

## ðŸ› ï¸ Implementation Roadmap

### Phase 1: Adapt Foundation (Week 1-2)
**Goal**: Handle 70% of queries with Adapt

1. **Define Core Intents** (2 days)
   - Machine status (is X running?)
   - Energy query (X consumption)
   - Cost query (cost today)
   - Simple ranking (top 3)
   - Anomaly check (any alerts?)

2. **Entity Extraction** (2 days)
   - Machine names (all 8 machines + variations)
   - Time ranges (today, yesterday, last 24h, etc.)
   - Metrics (kwh, kw, watts, energy, power)
   - Numbers (1, 2, 3, ... for top N)

3. **Skill Development** (3 days)
   - Map intents â†’ ENMS API endpoints
   - Handle API responses
   - Format voice responses

4. **Testing** (2 days)
   - Test with 100 sample queries
   - Measure accuracy and coverage

**Deliverable**: Working OVOS skill with 70% query coverage

---

### Phase 2: Heuristic Enhancement (Week 3)
**Goal**: Boost coverage to 80% without LLM

1. **Pattern Analysis** (1 day)
   - Analyze failed queries from Phase 1
   - Identify common patterns

2. **Heuristic Router** (2 days)
   - Top N detection (regex)
   - Comparison detection (keywords)
   - Factory-wide detection
   - Multi-entity extraction

3. **Fallback Messages** (1 day)
   - "I didn't understand, did you mean...?"
   - Suggest alternatives

4. **Testing** (1 day)
   - Re-test with updated logic

**Deliverable**: 80% coverage with fast responses

---

### Phase 3: LLM Integration (Week 4)
**Goal**: 95%+ coverage for complex queries

1. **Model Selection** (1 day)
   - Download Qwen2.5-1.5B-Instruct-Q4
   - Benchmark inference speed on target hardware
   - Compare with 3B model

2. **Prompt Engineering** (2 days)
   - Design system prompt
   - Test entity extraction accuracy
   - Handle edge cases

3. **Integration** (1 day)
   - Add LLM as Tier 3 fallback
   - Implement timeout and error handling
   - Cache model in memory

4. **Testing & Optimization** (1 day)
   - Full test suite (118 queries)
   - Optimize prompt for speed
   - Measure end-to-end latency

**Deliverable**: Production-ready system with 95% coverage

---

### Phase 4: Monitoring & Refinement (Ongoing)
1. **Usage Analytics**
   - Log which tier handles each query
   - Track response times
   - Monitor error rates

2. **Pattern Migration**
   - Promote common LLM queries to Adapt rules
   - Reduce LLM dependency over time

3. **User Feedback**
   - Collect accuracy feedback
   - Add new patterns based on real usage

---

## ðŸš§ Challenges & Solutions

### Challenge 1: LLM Hallucination
**Problem**: LLM may invent machine names or API endpoints

**Solutions**:
1. **Constrained Decoding**: Force output to match schema
2. **Validation Layer**: Check extracted entities against known lists
3. **Confidence Scoring**: Reject low-confidence outputs
4. **Fallback to Clarification**: "Did you mean Compressor-1 or Compressor-EU-1?"

```python
def validate_llm_output(output, known_machines):
    if output["entities"]["machine"] not in known_machines:
        return {"error": "unknown_machine", "suggestions": find_similar(output["entities"]["machine"], known_machines)}
    return output
```

---

### Challenge 2: Latency on CPU
**Problem**: 500ms-1s is slow for voice UX

**Solutions**:
1. **Model Quantization**: Use 4-bit (2x faster than 8-bit)
2. **Prompt Optimization**: Keep prompts short (<200 tokens)
3. **Caching**: Cache model in memory, avoid reload
4. **Progress Indicator**: "Let me check..." (speech during LLM processing)
5. **Async Processing**: Don't block user input

```python
async def handle_query(query):
    # Show thinking indicator
    speak_async("Let me check that for you...")
    
    # Process with LLM
    intent = await llm_parse(query)
    
    # Execute and respond
    result = await call_api(intent)
    speak(format_response(result))
```

---

### Challenge 3: Memory Constraints
**Problem**: 3GB for LLM on Pi 4 (4GB total)

**Solutions**:
1. **Lazy Loading**: Load LLM on first complex query
2. **Model Swap**: Unload LLM after 60s idle
3. **Lighter Model**: Try Qwen2.5-0.5B (experimental)
4. **Alternative Hardware**: Recommend Pi 5 (8GB) for production

---

### Challenge 4: Ambiguous Queries
**Problem**: "top 3", "watts" without context

**Solutions**:
1. **Clarification Dialog**:
   - User: "top 3"
   - OVOS: "Top 3 machines by energy consumption, cost, or anomalies?"
   
2. **Context from Previous Query**:
   - User: "Compressor-1 energy"
   - User: "What about Boiler-1?" (context: energy)

3. **Default Assumptions**:
   - "top 3" â†’ top 3 by energy (most common)
   - "watts" â†’ factory-wide current power

---

## ðŸ’° Alternative: Pure LLM with Optimization

If you decide **LLM only** is worth the trade-offs, here's how to optimize:

### Recommended Model: Qwen2.5-1.5B-Instruct
- **Size**: ~1GB (4-bit quantization)
- **RAM**: 2-3GB
- **CPU Latency**: 500-800ms (4-core ARM/x86)
- **Quality**: Excellent for instruction following

### Speed Optimizations
1. **Speculative Decoding**: 1.5-2x faster
2. **Flash Attention**: 20-30% faster
3. **llama.cpp**: Optimized C++ inference (~400ms)
4. **ONNX Runtime**: Cross-platform optimization

```bash
# Install optimized inference
pip install llama-cpp-python
# or
pip install optimum[onnxruntime]

# Benchmark
python benchmark_llm.py --model Qwen2.5-1.5B-Instruct-Q4_K_M.gguf
# Expected: 400-600ms on decent CPU
```

### Prompt Engineering for Speed
```python
# âŒ Bad: Verbose prompt (slow)
"You are a helpful assistant for energy management. Please analyze the following query carefully and extract the intent, entities, and parameters. Be thorough and accurate."

# âœ… Good: Concise prompt (fast)
"Extract intent & entities as JSON. User: {query}"
```

---

## ðŸ“ˆ Success Metrics

### Define Success Before Implementation
1. **Accuracy**: 90%+ correct intent detection
2. **Speed**: 80% queries <500ms end-to-end
3. **Coverage**: 95%+ queries handled (not "I don't understand")
4. **User Satisfaction**: >4/5 rating after 100 queries

### Testing Plan
1. **Unit Tests**: 118 queries from test-questions.md
2. **Integration Tests**: Full OVOS â†’ ENMS API â†’ Response
3. **User Testing**: 10 users Ã— 20 queries each
4. **Stress Testing**: 100 queries/minute sustained

---

## ðŸŽ“ Learning Resources

### Adapt Intent Parser
- OVOS Docs: https://openvoiceos.github.io/ovos-technical-manual/
- Adapt GitHub: https://github.com/MycroftAI/adapt
- Tutorial: Building OVOS Skills with Adapt

### LLM Integration
- Qwen2.5 Models: https://huggingface.co/Qwen
- llama.cpp: https://github.com/ggerganov/llama.cpp
- OVOS LLM Plugin: (check community plugins)

### Hybrid Approaches
- Rasa NLU (similar hybrid): https://rasa.com/docs/rasa/nlu/
- Snips NLU (discontinued but good reference)

---

## ðŸŽ¯ Final Recommendation

### **Go with Hybrid Approach - Here's Why**

1. **Your queries have clear tiers**:
   - 40% simple â†’ Adapt handles perfectly
   - 35% medium â†’ Adapt with heuristics
   - 25% complex â†’ LLM required

2. **Speed is critical**:
   - Voice UX demands <1s response
   - Adapt gives you <10ms for 80% of queries
   - LLM only for 20% = better average latency

3. **Resource constraints**:
   - No GPU = CPU inference only
   - Hybrid minimizes CPU usage
   - LLM only when needed

4. **Development pragmatism**:
   - Start with Adapt (2 weeks) â†’ working product
   - Add LLM later (1 week) â†’ enhanced product
   - Can ship Phase 1 and iterate

5. **Open source + local**:
   - Both Adapt and Qwen2.5 are open source
   - Fully offline operation
   - Meets WASABI EU privacy requirements

### Concrete Next Steps

**Week 1-2**: Build Adapt-based OVOS skill
- Define 10 core intents
- Extract entities (machines, time, metrics)
- Connect to ENMS API
- Test with 70% of queries
- **Deliverable**: Working voice assistant (limited)

**Week 3**: Add heuristics
- Pattern-based routing
- Clarification dialogs
- Boost to 80% coverage
- **Deliverable**: Production-ready v1.0

**Week 4**: Integrate Qwen2.5-1.5B (optional)
- Add LLM fallback
- Prompt engineering
- Testing
- **Deliverable**: Enhanced v2.0 (95% coverage)

**Total Time**: 3-4 weeks to full solution

---

## â“ Questions to Ask Yourself

Before deciding, answer these:

1. **What's your target hardware?**
   - Pi 4 (4GB) â†’ Hybrid or Adapt-only
   - Pi 5 (8GB) â†’ Hybrid works great
   - x86 server (16GB+) â†’ Hybrid or LLM-only

2. **What's acceptable latency?**
   - <500ms â†’ Adapt-only or aggressive caching
   - <1s â†’ Hybrid (recommended)
   - <2s â†’ LLM-only acceptable

3. **How much dev time do you have?**
   - 2 weeks â†’ Adapt-only
   - 3-4 weeks â†’ Hybrid (recommended)
   - 1 week â†’ LLM-only (faster dev, but slower runtime)

4. **How will queries evolve?**
   - Fixed patterns â†’ Adapt-only
   - Growing complexity â†’ Hybrid
   - Unknown/exploratory â†’ LLM-only

5. **What's your maintenance capacity?**
   - Low â†’ LLM-only (less manual rules)
   - Medium â†’ Hybrid (balanced)
   - High â†’ Adapt-only (full control)

---

## ðŸ“ž Conclusion

For WASABI's ENMS + OVOS integration, the **hybrid approach offers the best ROI**:
- Fast enough (80% queries <10ms)
- Accurate enough (95%+ coverage)
- Resource-efficient (CPU-only, 3GB RAM)
- Pragmatic (ship v1.0 in 2 weeks, enhance later)
- Open source (Adapt + Qwen2.5)
- Fully local (no cloud dependency)

**Start with Adapt, add LLM selectively.** This lets you ship quickly while maintaining room for enhancement.

---

**Need help with implementation?** I can:
1. Generate Adapt intent definitions for your ENMS API
2. Write heuristic router code
3. Create LLM system prompts optimized for your use case
4. Benchmark Qwen2.5 models on your hardware
5. Build a complete OVOS skill prototype

Just let me know! ðŸš€
